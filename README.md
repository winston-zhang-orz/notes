# LLM Serving Frameworks


| Framework         | Description                                                                 | GitHub Link                                     | Performance Score |
|-------------------|-----------------------------------------------------------------------------|-------------------------------------------------|-------------------------------|
| FlexFlow Serve    | Low-Latency, High-Performance LLM Serving                                   | [flexflow/FlexFlow](https://github.com/flexflow/FlexFlow) | 9                             |
| TensorFlow Serving| Flexible, high-performance serving system for machine learning models       | [tensorflow/serving](https://github.com/tensorflow/serving) | 8                             |
| NVIDIA Triton     | High-performance inference server for AI models                             | [triton-inference-server/server](https://github.com/triton-inference-server/server) | 9                             |
| TorchServe        | Serve PyTorch models at production scale                                    | [pytorch/serve](https://github.com/pytorch/serve) | 7                             |
| ONNX Runtime      | Cross-platform, high-performance scoring engine for Open Neural Network Exchange (ONNX) models | [microsoft/onnxruntime](https://github.com/microsoft/onnxruntime) | 8                             |
| Ray Serve         | Scalable model serving built on Ray                                         | [ray-project/ray](https://github.com/ray-project/ray) | 8                             |
| PyTorch Serving   | High-performance serving framework for PyTorch models                       | [pytorch/serve](https://github.com/pytorch/serve) | 7                             |


# Feature Flags
https://zhuanlan.zhihu.com/p/496746255#:~:text=%E4%BB%8EFeature%20f
